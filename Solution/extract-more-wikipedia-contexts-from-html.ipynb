{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install wikipedia-api","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-10-06T14:23:26.320115Z","iopub.execute_input":"2023-10-06T14:23:26.320722Z","iopub.status.idle":"2023-10-06T14:23:38.708096Z","shell.execute_reply.started":"2023-10-06T14:23:26.32069Z","shell.execute_reply":"2023-10-06T14:23:38.706699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import wikipediaapi","metadata":{"execution":{"iopub.status.busy":"2023-10-06T14:31:49.441956Z","iopub.execute_input":"2023-10-06T14:31:49.442317Z","iopub.status.idle":"2023-10-06T14:31:49.517333Z","shell.execute_reply.started":"2023-10-06T14:31:49.442288Z","shell.execute_reply":"2023-10-06T14:31:49.515964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wiki_html = wikipediaapi.Wikipedia(\n    user_agent='MyProjectName (merlin@example.com)',\n    language='en',\n    extract_format=wikipediaapi.ExtractFormat.HTML\n)\n# p_html = wiki_html.page(\"(332446) 2008 AF4\")\n# print(p_html.text)","metadata":{"execution":{"iopub.status.busy":"2023-10-06T16:51:01.142365Z","iopub.execute_input":"2023-10-06T16:51:01.142774Z","iopub.status.idle":"2023-10-06T16:51:01.30599Z","shell.execute_reply.started":"2023-10-06T16:51:01.142742Z","shell.execute_reply":"2023-10-06T16:51:01.304733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!cp /kaggle/input/datasets-wheel/datasets-2.14.4-py3-none-any.whl /kaggle/working\n!pip install  /kaggle/working/datasets-2.14.4-py3-none-any.whl\n!cp -r /kaggle/input/stem-wiki-cohere-no-emb /kaggle/working\n!cp -r /kaggle/input/all-paraphs-parsed-expanded /kaggle/working/","metadata":{"execution":{"iopub.status.busy":"2023-10-06T14:56:45.111945Z","iopub.execute_input":"2023-10-06T14:56:45.112355Z","iopub.status.idle":"2023-10-06T14:57:23.272829Z","shell.execute_reply.started":"2023-10-06T14:56:45.112328Z","shell.execute_reply":"2023-10-06T14:57:23.271014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datasets import load_from_disk\n#查看最终是什么样的\ncohere_dataset_filtered = load_from_disk(\"/kaggle/input/all-paraphs-parsed-expanded\")\n\n#数据的text表示内容，section表示HTML中的标签\n#所以一个title 会有许多个section，一个section会有多个text\n\ndf = cohere_dataset_filtered.to_pandas()","metadata":{"execution":{"iopub.status.busy":"2023-10-05T16:22:39.6505Z","iopub.execute_input":"2023-10-05T16:22:39.650909Z","iopub.status.idle":"2023-10-05T16:22:47.720579Z","shell.execute_reply.started":"2023-10-05T16:22:39.650865Z","shell.execute_reply":"2023-10-05T16:22:47.71938Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2023-10-05T16:23:03.955669Z","iopub.execute_input":"2023-10-05T16:23:03.956119Z","iopub.status.idle":"2023-10-05T16:23:03.97196Z","shell.execute_reply.started":"2023-10-05T16:23:03.956079Z","shell.execute_reply":"2023-10-05T16:23:03.970917Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# stem-wiki-cohere-no-emb id\ttitle\ttext\turl\twiki_id\tviews\tparagraph_id\tlangs\n# all-paraphs-parsed-expanded 'title', 'section', 'text'\n\n# all-paraphs-parsed-expanded 是基于stem-wiki-cohere-no-emb 通过wikipedia api进行重新请求的。\n# 逻辑是：输入title，请求对应的full page。section 表示为page中的段落，例如history ，Examples。\n# text 是对应的内容","metadata":{"execution":{"iopub.status.busy":"2023-10-05T03:40:58.403661Z","iopub.execute_input":"2023-10-05T03:40:58.404086Z","iopub.status.idle":"2023-10-05T03:40:58.418951Z","shell.execute_reply.started":"2023-10-05T03:40:58.404054Z","shell.execute_reply":"2023-10-05T03:40:58.417105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cohere_dataset = load_from_disk(\"/kaggle/input/new-wiki-15-clusters\")","metadata":{"execution":{"iopub.status.busy":"2023-10-06T16:51:52.184529Z","iopub.execute_input":"2023-10-06T16:51:52.184954Z","iopub.status.idle":"2023-10-06T16:51:52.473844Z","shell.execute_reply.started":"2023-10-06T16:51:52.184923Z","shell.execute_reply":"2023-10-06T16:51:52.472582Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cohere_dataset = cohere_dataset.to_pandas()","metadata":{"execution":{"iopub.status.busy":"2023-10-06T16:51:53.128298Z","iopub.execute_input":"2023-10-06T16:51:53.128751Z","iopub.status.idle":"2023-10-06T16:52:07.378905Z","shell.execute_reply.started":"2023-10-06T16:51:53.128717Z","shell.execute_reply":"2023-10-06T16:52:07.377839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install beautifulsoup4\nfrom bs4 import BeautifulSoup\n\nfrom tqdm import tqdm\nimport pandas as pd\nimport regex as re\n\ndataframe = pd.DataFrame(columns=[\"title\", \"section\", \"text\"])\n\nunique_titles = cohere_dataset[\"title\"].unique()\nprint(\"The number of different titles:\", len(unique_titles))\nprint(\"5 titles example: \", unique_titles[:5])\n\ndef clean_string(s):\n    text = re.sub(r'#+', '', s)\n    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n    text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n    text = re.sub(r'\\(\\)', '', text)\n    text = re.sub(r'\\s+', ' ', text).strip()\n    return text\n\ndef extract_section_and_text(title, html_content):\n    \n    extracted_data = []\n    \n    soup = BeautifulSoup(html_content, 'html.parser')\n    \n    first_h_tag = soup.find(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])\n    prev_p_tags = first_h_tag.find_all_previous('p') if first_h_tag else []\n    if prev_p_tags:\n        for p in prev_p_tags:\n            text = p.get_text()\n            text = clean_string(text)\n            extracted_data.append({\"title\": title, \"section\": \"start\", \"text\": text})\n            \n    for tag in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6']):\n        section = tag.get_text()\n        \n        exclude_section = [\"See also\", \"References\", \"Further reading\", \"Bibliography\"]\n        \n        if section not in exclude_section:\n            for sibling in tag.find_next_siblings():\n                if sibling.name and sibling.name.startswith('h'):\n                    break\n                if sibling.name == 'p':\n                    text = sibling.get_text()\n                    text = clean_string(text)\n                    extracted_data.append({\"title\": title, \"section\": section, \"text\": text})\n                \n    last_h_tag = soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])[-1] if soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6']) else None\n    next_p_tags = last_h_tag.find_all_next('p') if last_h_tag else []\n    if next_p_tags:\n        for p in next_p_tags:\n            text = p.get_text()\n            text = clean_string(text)\n            extracted_data.append({\"title\": title, \"section\": \"end\", \"text\": text})\n\n    return extracted_data","metadata":{"execution":{"iopub.status.busy":"2023-10-06T16:50:47.333318Z","iopub.execute_input":"2023-10-06T16:50:47.333854Z","iopub.status.idle":"2023-10-06T16:50:58.462578Z","shell.execute_reply.started":"2023-10-06T16:50:47.333816Z","shell.execute_reply":"2023-10-06T16:50:58.460943Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 创建一个多线程池\nimport pandas as pd\nimport concurrent.futures\nimport requests\nimport json\nfrom tqdm import tqdm\n\ns = 8\ndf = unique_titles[50000*(s-1):50000*(s)]\n#df = unique_titles[84400:50000*s]\nout = []\n#df = unique_titles[:100]\nmax_threads = 20  # 指定最大线程数\ndataframe = pd.DataFrame(columns=[\"title\", \"section\", \"text\"])\n\nwith concurrent.futures.ThreadPoolExecutor(max_threads) as executor:\n    # 定义一个函数来处理每一行的数据\n    def get_and_clean(use_title):\n        try:\n            p_html = wiki_html.page(use_title)\n            extracted_data = extract_section_and_text(use_title, p_html.text)\n            return extracted_data\n        except:\n            out.append(use_title)\n            print(f\"error {use_title}\")\n            return [{\"title\": None, \"section\": None, \"text\": None}]\n        \n    # 使用多线程处理每一行        \n    for result in tqdm(executor.map(get_and_clean, df), total=len(df)):\n        try:\n            dataframe = pd.concat([dataframe, pd.DataFrame(result)], ignore_index=True)\n        except:\n            print(f\"error title\")\n            print(result)","metadata":{"execution":{"iopub.status.busy":"2023-10-06T16:52:22.018329Z","iopub.execute_input":"2023-10-06T16:52:22.018773Z","iopub.status.idle":"2023-10-06T16:56:09.698712Z","shell.execute_reply.started":"2023-10-06T16:52:22.018739Z","shell.execute_reply":"2023-10-06T16:56:09.697713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(out,columns = ['title']).to_csv('out.csv',index = False)","metadata":{"execution":{"iopub.status.busy":"2023-10-05T14:58:49.366432Z","iopub.execute_input":"2023-10-05T14:58:49.366937Z","iopub.status.idle":"2023-10-05T14:58:49.383745Z","shell.execute_reply.started":"2023-10-05T14:58:49.366901Z","shell.execute_reply":"2023-10-05T14:58:49.381674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#保存为dataset\nimport datasets\ndf = datasets.Dataset.from_pandas(dataframe.reset_index(drop=True))\ndf.save_to_disk(f'new_wiki_emb_20_clusters_{50000*(s-1)}_{50000*(s)}')","metadata":{"execution":{"iopub.status.busy":"2023-10-05T09:48:29.821105Z","iopub.execute_input":"2023-10-05T09:48:29.821537Z","iopub.status.idle":"2023-10-05T09:48:29.863769Z","shell.execute_reply.started":"2023-10-05T09:48:29.821506Z","shell.execute_reply":"2023-10-05T09:48:29.863077Z"},"trusted":true},"execution_count":null,"outputs":[]}]}