{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -U /kaggle/input/faiss-gpu-173-python310/faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n!cp -rf /kaggle/input/sentence-transformers-222/sentence-transformers /kaggle/working/sentence-transformers\n!pip install -U /kaggle/working/sentence-transformers\n!pip install -U /kaggle/input/blingfire-018/blingfire-0.1.8-py3-none-any.whl\n\n!pip install --no-index --no-deps /kaggle/input/llm-whls/transformers-4.31.0-py3-none-any.whl\n!pip install --no-index --no-deps /kaggle/input/llm-whls/peft-0.4.0-py3-none-any.whl\n!pip install --no-index --no-deps /kaggle/input/llm-whls/datasets-2.14.3-py3-none-any.whl\n!pip install --no-index --no-deps /kaggle/input/llm-whls/trl-0.5.0-py3-none-any.whl","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-09-04T03:19:39.214252Z","iopub.execute_input":"2023-09-04T03:19:39.215367Z","iopub.status.idle":"2023-09-04T03:21:25.505002Z","shell.execute_reply.started":"2023-09-04T03:19:39.215317Z","shell.execute_reply":"2023-09-04T03:21:25.503769Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport gc\nimport pandas as pd\nimport numpy as np\nimport re\nfrom tqdm.auto import tqdm\nimport blingfire as bf\nfrom __future__ import annotations\nfrom collections.abc import Iterable\nimport faiss\nfrom faiss import write_index, read_index\nfrom sentence_transformers import SentenceTransformer\nimport matplotlib.pyplot as plt\n\nimport torch\nimport ctypes\nlibc = ctypes.CDLL(\"libc.so.6\")\nfrom dataclasses import dataclass\nfrom typing import Optional, Union\nfrom datasets import Dataset\nfrom transformers import AutoTokenizer\nfrom transformers import AutoModelForMultipleChoice, TrainingArguments, Trainer\nfrom transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\nfrom torch.utils.data import DataLoader\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2023-09-04T03:21:25.507692Z","iopub.execute_input":"2023-09-04T03:21:25.508078Z","iopub.status.idle":"2023-09-04T03:21:38.006081Z","shell.execute_reply.started":"2023-09-04T03:21:25.508039Z","shell.execute_reply":"2023-09-04T03:21:38.005126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def process_documents(documents: Iterable[str],\n                      document_ids: Iterable,\n                      split_sentences: bool = True,\n                      filter_len: int = 9,\n                      disable_progress_bar: bool = False) -> pd.DataFrame:\n    \"\"\"\n    Main helper function to process documents from the EMR.\n\n    :param documents: Iterable containing documents which are strings\n    :param document_ids: Iterable containing document unique identifiers\n    :param document_type: String denoting the document type to be processed\n    :param document_sections: List of sections for a given document type to process\n    :param split_sentences: Flag to determine whether to further split sections into sentences\n    :param filter_len: Minimum character length of a sentence (otherwise filter out)\n    :param disable_progress_bar: Flag to disable tqdm progress bar\n    :return: Pandas DataFrame containing the columns `document_id`, `text`, `section`, `offset`\n    \"\"\"\n    \n    df = sectionize_documents(documents, document_ids, disable_progress_bar)\n\n    if split_sentences:\n        df = sentencize(df.text.values, \n                        df.document_id.values,\n                        df.offset.values, \n                        filter_len, \n                        disable_progress_bar)\n    return df","metadata":{"execution":{"iopub.status.busy":"2023-09-04T03:21:54.263381Z","iopub.execute_input":"2023-09-04T03:21:54.264295Z","iopub.status.idle":"2023-09-04T03:21:54.272655Z","shell.execute_reply.started":"2023-09-04T03:21:54.264258Z","shell.execute_reply":"2023-09-04T03:21:54.271597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def sectionize_documents(documents: Iterable[str],\n                         document_ids: Iterable,\n                         disable_progress_bar: bool = False) -> pd.DataFrame:\n    \"\"\"\n    Obtains the sections of the imaging reports and returns only the \n    selected sections (defaults to FINDINGS, IMPRESSION, and ADDENDUM).\n\n    :param documents: Iterable containing documents which are strings\n    :param document_ids: Iterable containing document unique identifiers\n    :param disable_progress_bar: Flag to disable tqdm progress bar\n    :return: Pandas DataFrame containing the columns `document_id`, `text`, `offset`\n    \"\"\"\n    processed_documents = []\n    for document_id, document in tqdm(zip(document_ids, documents), total=len(documents), disable=disable_progress_bar):\n        row = {}\n        text, start, end = (document, 0, len(document))\n        row['document_id'] = document_id\n        row['text'] = text\n        row['offset'] = (start, end)\n\n        processed_documents.append(row)\n\n    _df = pd.DataFrame(processed_documents)\n    if _df.shape[0] > 0:\n        return _df.sort_values(['document_id', 'offset']).reset_index(drop=True)\n    else:\n        return _df","metadata":{"execution":{"iopub.status.busy":"2023-09-04T03:21:54.274336Z","iopub.execute_input":"2023-09-04T03:21:54.275008Z","iopub.status.idle":"2023-09-04T03:21:54.289346Z","shell.execute_reply.started":"2023-09-04T03:21:54.274973Z","shell.execute_reply":"2023-09-04T03:21:54.288309Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def sentencize(documents: Iterable[str],\n               document_ids: Iterable,\n               offsets: Iterable[tuple[int, int]],\n               filter_len: int = 9,\n               disable_progress_bar: bool = False) -> pd.DataFrame:\n    \"\"\"\n    Split a document into sentences. Can be used with `sectionize_documents`\n    to further split documents into more manageable pieces. Takes in offsets\n    to ensure that after splitting, the sentences can be matched to the\n    location in the original documents.\n\n    :param documents: Iterable containing documents which are strings\n    :param document_ids: Iterable containing document unique identifiers\n    :param offsets: Iterable tuple of the start and end indices\n    :param filter_len: Minimum character length of a sentence (otherwise filter out)\n    :return: Pandas DataFrame containing the columns `document_id`, `text`, `section`, `offset`\n    \"\"\"\n\n    document_sentences = []\n    for document, document_id, offset in tqdm(zip(documents, document_ids, offsets), total=len(documents), disable=disable_progress_bar):\n        try:\n            _, sentence_offsets = bf.text_to_sentences_and_offsets(document)\n            for o in sentence_offsets:\n                if o[1]-o[0] > filter_len:\n                    sentence = document[o[0]:o[1]]\n                    abs_offsets = (o[0]+offset[0], o[1]+offset[0])\n                    row = {}\n                    row['document_id'] = document_id\n                    row['text'] = sentence\n                    row['offset'] = abs_offsets\n                    document_sentences.append(row)\n        except:\n            continue\n    return pd.DataFrame(document_sentences)","metadata":{"execution":{"iopub.status.busy":"2023-09-04T03:21:54.29267Z","iopub.execute_input":"2023-09-04T03:21:54.292943Z","iopub.status.idle":"2023-09-04T03:21:54.304788Z","shell.execute_reply.started":"2023-09-04T03:21:54.29292Z","shell.execute_reply":"2023-09-04T03:21:54.303851Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SIM_MODEL = '/kaggle/input/sentence-transformer-model/gte-small'\nDEVICE = 0\nMAX_LENGTH = 384\nBATCH_SIZE = 32\nWIKI_PATH = \"/kaggle/input/wikipedia-20230701\"\nwiki_files = os.listdir(WIKI_PATH)","metadata":{"execution":{"iopub.status.busy":"2023-09-04T03:21:54.3078Z","iopub.execute_input":"2023-09-04T03:21:54.308095Z","iopub.status.idle":"2023-09-04T03:21:54.325887Z","shell.execute_reply.started":"2023-09-04T03:21:54.308072Z","shell.execute_reply":"2023-09-04T03:21:54.324954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trn = pd.read_csv(\"\")\ntrn = trn[[\"prompt\", \"A\", \"B\", \"C\", \"D\", \"E\", \"answer\"]]\ntrn.insert(0, \"id\", range(len(trn)))\ntrn = trn.fillna(\"\")\n# trn = trn[:]\ntrn = trn.reset_index(drop=True)\ntrn.head()","metadata":{"execution":{"iopub.status.busy":"2023-10-08T04:01:34.527536Z","iopub.execute_input":"2023-10-08T04:01:34.52787Z","iopub.status.idle":"2023-10-08T04:01:34.555571Z","shell.execute_reply.started":"2023-10-08T04:01:34.527846Z","shell.execute_reply":"2023-10-08T04:01:34.55405Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trn['answer_all'] = trn.apply(lambda x: \" \".join([str(x['A']), str(x['B']), str(x['C']), str(x['D']), str(x['E'])]), axis=1)\n\ntrn['prompt_answer_stem'] = trn['prompt'] + \" \" + trn['answer_all']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = SentenceTransformer(SIM_MODEL, device='cuda')\nmodel.max_seq_length = MAX_LENGTH","metadata":{"execution":{"iopub.status.busy":"2023-09-04T03:21:54.353322Z","iopub.execute_input":"2023-09-04T03:21:54.353697Z","iopub.status.idle":"2023-09-04T03:21:56.459215Z","shell.execute_reply.started":"2023-09-04T03:21:54.353657Z","shell.execute_reply":"2023-09-04T03:21:56.458206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sentence_index = read_index(\"/kaggle/input/wikipedia-faiss-index/merged.index\")","metadata":{"execution":{"iopub.status.busy":"2023-09-04T03:21:56.460754Z","iopub.execute_input":"2023-09-04T03:21:56.4611Z","iopub.status.idle":"2023-09-04T03:23:35.981778Z","shell.execute_reply.started":"2023-09-04T03:21:56.461066Z","shell.execute_reply":"2023-09-04T03:23:35.980718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prompt_embeddings = model.encode(trn.prompt_answer_stem.values, batch_size=BATCH_SIZE, device=DEVICE, show_progress_bar=True, convert_to_tensor=True, normalize_embeddings=True)\nprompt_embeddings = prompt_embeddings.detach().cpu().numpy()\n_ = gc.collect()","metadata":{"execution":{"iopub.status.busy":"2023-09-04T03:23:35.98438Z","iopub.execute_input":"2023-09-04T03:23:35.984766Z","iopub.status.idle":"2023-09-04T03:23:44.41658Z","shell.execute_reply.started":"2023-09-04T03:23:35.984731Z","shell.execute_reply":"2023-09-04T03:23:44.415567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"search_score, search_index = sentence_index.search(prompt_embeddings, 10)","metadata":{"execution":{"iopub.status.busy":"2023-09-04T03:23:44.422327Z","iopub.execute_input":"2023-09-04T03:23:44.425702Z","iopub.status.idle":"2023-09-04T03:24:05.176628Z","shell.execute_reply.started":"2023-09-04T03:23:44.425664Z","shell.execute_reply":"2023-09-04T03:24:05.175794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del sentence_index\ndel prompt_embeddings\n_ = gc.collect()\nlibc.malloc_trim(0)","metadata":{"execution":{"iopub.status.busy":"2023-09-04T03:24:05.181827Z","iopub.execute_input":"2023-09-04T03:24:05.183736Z","iopub.status.idle":"2023-09-04T03:24:06.104894Z","shell.execute_reply.started":"2023-09-04T03:24:05.183707Z","shell.execute_reply":"2023-09-04T03:24:06.103889Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_parquet(\"/kaggle/input/wikipedia-20230701/wiki_2023_index.parquet\",\n                     columns=['id', 'file'])","metadata":{"execution":{"iopub.status.busy":"2023-09-04T03:24:06.106247Z","iopub.execute_input":"2023-09-04T03:24:06.107161Z","iopub.status.idle":"2023-09-04T03:24:11.686737Z","shell.execute_reply.started":"2023-09-04T03:24:06.107122Z","shell.execute_reply":"2023-09-04T03:24:11.68577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wikipedia_file_data = []\n\nfor i, (scr, idx) in tqdm(enumerate(zip(search_score, search_index)), total=len(search_score)):\n    scr_idx = idx\n    _df = df.loc[scr_idx].copy()\n    _df['prompt_id'] = i\n    wikipedia_file_data.append(_df)\nwikipedia_file_data = pd.concat(wikipedia_file_data).reset_index(drop=True)\nwikipedia_file_data = wikipedia_file_data[['id', 'prompt_id', 'file']].drop_duplicates().sort_values(['file', 'id']).reset_index(drop=True)\n\n## Save memory - delete df since it is no longer necessary\ndel df\n_ = gc.collect()\nlibc.malloc_trim(0)","metadata":{"execution":{"iopub.status.busy":"2023-09-04T03:24:11.688044Z","iopub.execute_input":"2023-09-04T03:24:11.688389Z","iopub.status.idle":"2023-09-04T03:24:12.526394Z","shell.execute_reply.started":"2023-09-04T03:24:11.688358Z","shell.execute_reply":"2023-09-04T03:24:12.525437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wiki_text_data = []\n\nfor file in tqdm(wikipedia_file_data.file.unique(), total=len(wikipedia_file_data.file.unique())):\n    _id = [str(i) for i in wikipedia_file_data[wikipedia_file_data['file']==file]['id'].tolist()]\n    _df = pd.read_parquet(f\"{WIKI_PATH}/{file}\", columns=['id', 'text'])\n\n    _df_temp = _df[_df['id'].isin(_id)].copy()\n    del _df\n    _ = gc.collect()\n    libc.malloc_trim(0)\n    wiki_text_data.append(_df_temp)\nwiki_text_data = pd.concat(wiki_text_data).drop_duplicates().reset_index(drop=True)\n_ = gc.collect()","metadata":{"execution":{"iopub.status.busy":"2023-09-04T03:24:12.52789Z","iopub.execute_input":"2023-09-04T03:24:12.528222Z","iopub.status.idle":"2023-09-04T03:29:03.522924Z","shell.execute_reply.started":"2023-09-04T03:24:12.528191Z","shell.execute_reply":"2023-09-04T03:29:03.521556Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"processed_wiki_text_data = process_documents(wiki_text_data.text.values, wiki_text_data.id.values)","metadata":{"execution":{"iopub.status.busy":"2023-09-04T03:29:03.526356Z","iopub.execute_input":"2023-09-04T03:29:03.526962Z","iopub.status.idle":"2023-09-04T03:29:11.158753Z","shell.execute_reply.started":"2023-09-04T03:29:03.526927Z","shell.execute_reply":"2023-09-04T03:29:11.15779Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wiki_data_embeddings = model.encode(processed_wiki_text_data.text,\n                                    batch_size=BATCH_SIZE,\n                                    device=DEVICE,\n                                    show_progress_bar=True,\n                                    convert_to_tensor=True,\n                                    normalize_embeddings=True)#.half()\nwiki_data_embeddings = wiki_data_embeddings.detach().cpu().numpy()","metadata":{"execution":{"iopub.status.busy":"2023-09-04T03:29:11.160327Z","iopub.execute_input":"2023-09-04T03:29:11.160722Z","iopub.status.idle":"2023-09-04T03:29:42.484327Z","shell.execute_reply.started":"2023-09-04T03:29:11.160687Z","shell.execute_reply":"2023-09-04T03:29:42.483294Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"_ = gc.collect()","metadata":{"execution":{"iopub.status.busy":"2023-09-04T03:29:42.48577Z","iopub.execute_input":"2023-09-04T03:29:42.486394Z","iopub.status.idle":"2023-09-04T03:29:42.852783Z","shell.execute_reply.started":"2023-09-04T03:29:42.48636Z","shell.execute_reply":"2023-09-04T03:29:42.85148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"question_embeddings = model.encode(trn.prompt_answer_stem.values, batch_size=BATCH_SIZE, device=DEVICE, show_progress_bar=True, convert_to_tensor=True, normalize_embeddings=True)\nquestion_embeddings = question_embeddings.detach().cpu().numpy()","metadata":{"execution":{"iopub.status.busy":"2023-09-04T03:29:42.876566Z","iopub.execute_input":"2023-09-04T03:29:42.876901Z","iopub.status.idle":"2023-09-04T03:29:43.237239Z","shell.execute_reply.started":"2023-09-04T03:29:42.87687Z","shell.execute_reply":"2023-09-04T03:29:43.236101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Parameter to determine how many relevant sentences to include\nNUM_SENTENCES_INCLUDE = 30\n\n## List containing just Context\ncontexts = []\nsep = \"\"\n\nfor r in tqdm(trn.itertuples(), total=len(trn)):\n\n    prompt_id = r.Index\n\n    prompt_indices = processed_wiki_text_data[processed_wiki_text_data['document_id'].isin(wikipedia_file_data[wikipedia_file_data['prompt_id']==prompt_id]['id'].values)].index.values\n\n    if prompt_indices.shape[0] > 0:\n        prompt_index = faiss.index_factory(wiki_data_embeddings.shape[1], \"Flat\")\n        prompt_index.add(wiki_data_embeddings[prompt_indices])\n\n        context = \"\"\n        \n        ## Get the top matches\n        ss, ii = prompt_index.search(question_embeddings, NUM_SENTENCES_INCLUDE)\n        for _s, _i in zip(ss[prompt_id], ii[prompt_id]):\n            context += (processed_wiki_text_data.loc[prompt_indices]['text'].iloc[_i] + sep)\n        \n    contexts.append(context)","metadata":{"execution":{"iopub.status.busy":"2023-09-04T03:29:43.238775Z","iopub.execute_input":"2023-09-04T03:29:43.239366Z","iopub.status.idle":"2023-09-04T03:29:47.20754Z","shell.execute_reply.started":"2023-09-04T03:29:43.239332Z","shell.execute_reply":"2023-09-04T03:29:47.206396Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trn['context'] = contexts","metadata":{"execution":{"iopub.status.busy":"2023-09-04T03:29:47.208938Z","iopub.execute_input":"2023-09-04T03:29:47.209372Z","iopub.status.idle":"2023-09-04T03:29:47.215414Z","shell.execute_reply.started":"2023-09-04T03:29:47.209338Z","shell.execute_reply":"2023-09-04T03:29:47.214458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trn[[\"prompt\", \"context\", \"A\", \"B\", \"C\", \"D\", \"E\", \"answer\"]].to_json(\"\", orient=\"records\")","metadata":{"execution":{"iopub.status.busy":"2023-09-04T03:29:47.217162Z","iopub.execute_input":"2023-09-04T03:29:47.217876Z","iopub.status.idle":"2023-09-04T03:29:47.286067Z","shell.execute_reply.started":"2023-09-04T03:29:47.217842Z","shell.execute_reply":"2023-09-04T03:29:47.285165Z"},"trusted":true},"execution_count":null,"outputs":[]}]}