{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -U /kaggle/input/faiss-gpu-173-python310/faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n!cp -rf /kaggle/input/sentence-transformers-222/sentence-transformers /kaggle/working/sentence-transformers\n!pip install -U /kaggle/working/sentence-transformers\n!pip install -U /kaggle/input/blingfire-018/blingfire-0.1.8-py3-none-any.whl\n\n!pip install --no-index --no-deps /kaggle/input/llm-whls/transformers-4.31.0-py3-none-any.whl\n!pip install --no-index --no-deps /kaggle/input/llm-whls/peft-0.4.0-py3-none-any.whl\n!pip install --no-index --no-deps /kaggle/input/llm-whls/datasets-2.14.3-py3-none-any.whl\n!pip install --no-index --no-deps /kaggle/input/llm-whls/trl-0.5.0-py3-none-any.whl","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-09-24T01:26:22.860401Z","iopub.execute_input":"2023-09-24T01:26:22.861324Z","iopub.status.idle":"2023-09-24T01:28:22.46642Z","shell.execute_reply.started":"2023-09-24T01:26:22.861285Z","shell.execute_reply":"2023-09-24T01:28:22.465244Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport gc\nimport pandas as pd\nimport numpy as np\nimport re\nfrom tqdm.auto import tqdm\nimport blingfire as bf\nfrom __future__ import annotations\nfrom collections.abc import Iterable\nimport faiss\nfrom faiss import write_index, read_index\nfrom sentence_transformers import SentenceTransformer\nimport matplotlib.pyplot as plt\n\nimport torch\nimport ctypes\nlibc = ctypes.CDLL(\"libc.so.6\")\nfrom dataclasses import dataclass\nfrom typing import Optional, Union\nfrom datasets import Dataset\nfrom transformers import AutoTokenizer\nfrom transformers import AutoModelForMultipleChoice, TrainingArguments, Trainer\nfrom transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\nfrom torch.utils.data import DataLoader\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport numpy as np\nimport pandas as pd \nfrom datasets import load_dataset, load_from_disk\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport torch\nfrom transformers import LongformerTokenizer, LongformerForMultipleChoice\nimport transformers\nimport pandas as pd\nimport pickle\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport unicodedata\n\nimport os","metadata":{"execution":{"iopub.status.busy":"2023-09-24T01:28:22.46907Z","iopub.execute_input":"2023-09-24T01:28:22.469527Z","iopub.status.idle":"2023-09-24T01:28:37.12114Z","shell.execute_reply.started":"2023-09-24T01:28:22.469455Z","shell.execute_reply":"2023-09-24T01:28:37.120179Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!cp /kaggle/input/datasets-wheel/datasets-2.14.4-py3-none-any.whl /kaggle/working\n!pip install  /kaggle/working/datasets-2.14.4-py3-none-any.whl\n!cp /kaggle/input/backup-806/util_openbook.py .","metadata":{"execution":{"iopub.status.busy":"2023-09-24T01:28:37.123354Z","iopub.execute_input":"2023-09-24T01:28:37.123647Z","iopub.status.idle":"2023-09-24T01:29:11.125606Z","shell.execute_reply.started":"2023-09-24T01:28:37.123622Z","shell.execute_reply":"2023-09-24T01:29:11.124247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!cp -r /kaggle/input/new-wiki-30-clusters-more /kaggle/working\n!cp -r /kaggle/input/all-paraphs-parsed-expanded /kaggle/working/","metadata":{"execution":{"iopub.status.busy":"2023-09-24T01:29:11.127974Z","iopub.execute_input":"2023-09-24T01:29:11.128397Z","iopub.status.idle":"2023-09-24T01:29:38.855479Z","shell.execute_reply.started":"2023-09-24T01:29:11.128356Z","shell.execute_reply":"2023-09-24T01:29:38.854132Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def SplitList(mylist, chunk_size):\n    return [mylist[offs:offs+chunk_size] for offs in range(0, len(mylist), chunk_size)]\n\ndef get_relevant_documents_parsed(df_valid):\n    df_chunk_size=600\n    paraphs_parsed_dataset = load_from_disk(\"/kaggle/working/all-paraphs-parsed-expanded\")\n    modified_texts = paraphs_parsed_dataset.map(lambda example:\n                                             {'temp_text':\n                                              f\"{example['title']} {example['section']} {example['text']}\".replace('\\n',\" \").replace(\"'\",\"\")},\n                                             num_proc=2)[\"temp_text\"]\n    \n    all_articles_indices = []\n    all_articles_values = []\n    for idx in tqdm(range(0, df_valid.shape[0], df_chunk_size)):\n        df_valid_ = df_valid.iloc[idx: idx+df_chunk_size]\n    \n        articles_indices, merged_top_scores = retrieval(df_valid_, modified_texts)\n        all_articles_indices.append(articles_indices)\n        all_articles_values.append(merged_top_scores)\n        \n    article_indices_array =  np.concatenate(all_articles_indices, axis=0)\n    articles_values_array = np.concatenate(all_articles_values, axis=0).reshape(-1)\n    \n    top_per_query = article_indices_array.shape[1]\n    articles_flatten = [(\n                         articles_values_array[index],\n                         paraphs_parsed_dataset[idx.item()][\"title\"],\n                         paraphs_parsed_dataset[idx.item()][\"text\"],\n                        )\n                        for index,idx in enumerate(article_indices_array.reshape(-1))]\n    retrieved_articles = SplitList(articles_flatten, top_per_query)\n    return retrieved_articles\n\n\n\ndef get_relevant_documents(df_valid):\n    df_chunk_size=800\n    \n    cohere_dataset_filtered = load_from_disk(\"/kaggle/working/new-wiki-30-clusters-more\")\n    modified_texts = cohere_dataset_filtered.map(lambda example:\n                                             {'temp_text':\n                                              unicodedata.normalize(\"NFKD\", f\"{example['title']} {example['text']}\").replace('\"',\"\")},\n                                             num_proc=2)[\"temp_text\"]\n    \n    all_articles_indices = []\n    all_articles_values = []\n    for idx in tqdm(range(0, df_valid.shape[0], df_chunk_size)):\n        df_valid_ = df_valid.iloc[idx: idx+df_chunk_size]\n    \n        articles_indices, merged_top_scores = retrieval(df_valid_, modified_texts)\n        all_articles_indices.append(articles_indices)\n        all_articles_values.append(merged_top_scores)\n        \n    article_indices_array =  np.concatenate(all_articles_indices, axis=0)\n    articles_values_array = np.concatenate(all_articles_values, axis=0).reshape(-1)\n    \n    top_per_query = article_indices_array.shape[1]\n    articles_flatten = [(\n                         articles_values_array[index],\n                         cohere_dataset_filtered[idx.item()][\"title\"],\n                         unicodedata.normalize(\"NFKD\", cohere_dataset_filtered[idx.item()][\"text\"]),\n                        )\n                        for index,idx in enumerate(article_indices_array.reshape(-1))]\n    retrieved_articles = SplitList(articles_flatten, top_per_query)\n    return retrieved_articles\n\n\n\ndef retrieval(df_valid, modified_texts):\n    \n    corpus_df_valid = df_valid.apply(lambda row:\n                                     f'{row[\"prompt\"]}\\n{row[\"prompt\"]}\\n{row[\"prompt\"]}\\n{row[\"A\"]}\\n{row[\"B\"]}\\n{row[\"C\"]}\\n{row[\"D\"]}\\n{row[\"E\"]}',\n                                     axis=1).values\n    vectorizer1 = TfidfVectorizer(ngram_range=(1,2),\n                                 token_pattern=r\"(?u)\\b[\\w/.-]+\\b|!|/|\\?|\\\"|\\'\",\n                                 stop_words=stop_words)\n    vectorizer1.fit(corpus_df_valid)\n    vocab_df_valid = vectorizer1.get_feature_names_out()\n    vectorizer = TfidfVectorizer(ngram_range=(1,2),\n                                 token_pattern=r\"(?u)\\b[\\w/.-]+\\b|!|/|\\?|\\\"|\\'\",\n                                 stop_words=stop_words,\n                                 vocabulary=vocab_df_valid)\n    vectorizer.fit(modified_texts[:500000])\n    corpus_tf_idf = vectorizer.transform(corpus_df_valid)\n    \n    print(f\"length of vectorizer vocab is {len(vectorizer.get_feature_names_out())}\")\n\n    chunk_size = 100000\n    top_per_chunk = 10\n    top_per_query = 10\n\n    all_chunk_top_indices = []\n    all_chunk_top_values = []\n\n    for idx in tqdm(range(0, len(modified_texts), chunk_size)):\n        wiki_vectors = vectorizer.transform(modified_texts[idx: idx+chunk_size])\n        temp_scores = (corpus_tf_idf * wiki_vectors.T).toarray()\n        chunk_top_indices = temp_scores.argpartition(-top_per_chunk, axis=1)[:, -top_per_chunk:]\n        chunk_top_values = temp_scores[np.arange(temp_scores.shape[0])[:, np.newaxis], chunk_top_indices]\n\n        all_chunk_top_indices.append(chunk_top_indices + idx)\n        all_chunk_top_values.append(chunk_top_values)\n\n    top_indices_array = np.concatenate(all_chunk_top_indices, axis=1)\n    top_values_array = np.concatenate(all_chunk_top_values, axis=1)\n    \n    merged_top_scores = np.sort(top_values_array, axis=1)[:,-top_per_query:]\n    merged_top_indices = top_values_array.argsort(axis=1)[:,-top_per_query:]\n    articles_indices = top_indices_array[np.arange(top_indices_array.shape[0])[:, np.newaxis], merged_top_indices]\n    \n    return articles_indices, merged_top_scores\n\n\ndef prepare_answering_input(\n        tokenizer, \n        question,  \n        options,   \n        context,   \n        max_seq_length=4096,\n    ):\n    c_plus_q   = context + ' ' + tokenizer.bos_token + ' ' + question\n    c_plus_q_4 = [c_plus_q] * len(options)\n    tokenized_examples = tokenizer(\n        c_plus_q_4, options,\n        max_length=max_seq_length,\n        padding=\"longest\",\n        truncation=False,\n        return_tensors=\"pt\",\n    )\n    input_ids = tokenized_examples['input_ids'].unsqueeze(0)\n    attention_mask = tokenized_examples['attention_mask'].unsqueeze(0)\n    example_encoded = {\n        \"input_ids\": input_ids.to(model.device.index),\n        \"attention_mask\": attention_mask.to(model.device.index),\n    }\n    return example_encoded\n","metadata":{"execution":{"iopub.status.busy":"2023-09-23T19:05:20.779803Z","iopub.execute_input":"2023-09-23T19:05:20.78018Z","iopub.status.idle":"2023-09-23T19:05:20.808638Z","shell.execute_reply.started":"2023-09-23T19:05:20.780144Z","shell.execute_reply":"2023-09-23T19:05:20.807394Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def process_documents(documents: Iterable[str],\n                      document_ids: Iterable,\n                      split_sentences: bool = True,\n                      filter_len: int = 9,\n                      disable_progress_bar: bool = False) -> pd.DataFrame:\n    \"\"\"\n    Main helper function to process documents from the EMR.\n\n    :param documents: Iterable containing documents which are strings\n    :param document_ids: Iterable containing document unique identifiers\n    :param document_type: String denoting the document type to be processed\n    :param document_sections: List of sections for a given document type to process\n    :param split_sentences: Flag to determine whether to further split sections into sentences\n    :param filter_len: Minimum character length of a sentence (otherwise filter out)\n    :param disable_progress_bar: Flag to disable tqdm progress bar\n    :return: Pandas DataFrame containing the columns `document_id`, `text`, `section`, `offset`\n    \"\"\"\n    \n    df = sectionize_documents(documents, document_ids, disable_progress_bar)\n\n    if split_sentences:\n        df = sentencize(df.text.values, \n                        df.document_id.values,\n                        df.offset.values, \n                        filter_len, \n                        disable_progress_bar)\n    return df","metadata":{"execution":{"iopub.status.busy":"2023-09-23T19:05:23.169258Z","iopub.execute_input":"2023-09-23T19:05:23.169922Z","iopub.status.idle":"2023-09-23T19:05:23.177402Z","shell.execute_reply.started":"2023-09-23T19:05:23.169888Z","shell.execute_reply":"2023-09-23T19:05:23.176316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def sectionize_documents(documents: Iterable[str],\n                         document_ids: Iterable,\n                         disable_progress_bar: bool = False) -> pd.DataFrame:\n    \"\"\"\n    Obtains the sections of the imaging reports and returns only the \n    selected sections (defaults to FINDINGS, IMPRESSION, and ADDENDUM).\n\n    :param documents: Iterable containing documents which are strings\n    :param document_ids: Iterable containing document unique identifiers\n    :param disable_progress_bar: Flag to disable tqdm progress bar\n    :return: Pandas DataFrame containing the columns `document_id`, `text`, `offset`\n    \"\"\"\n    processed_documents = []\n    for document_id, document in tqdm(zip(document_ids, documents), total=len(documents), disable=disable_progress_bar):\n        row = {}\n        text, start, end = (document, 0, len(document))\n        row['document_id'] = document_id\n        row['text'] = text\n        row['offset'] = (start, end)\n\n        processed_documents.append(row)\n\n    _df = pd.DataFrame(processed_documents)\n    if _df.shape[0] > 0:\n        return _df.sort_values(['document_id', 'offset']).reset_index(drop=True)\n    else:\n        return _df","metadata":{"execution":{"iopub.status.busy":"2023-09-23T19:05:24.426567Z","iopub.execute_input":"2023-09-23T19:05:24.427316Z","iopub.status.idle":"2023-09-23T19:05:24.438423Z","shell.execute_reply.started":"2023-09-23T19:05:24.42728Z","shell.execute_reply":"2023-09-23T19:05:24.437384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def sentencize(documents: Iterable[str],\n               document_ids: Iterable,\n               offsets: Iterable[tuple[int, int]],\n               filter_len: int = 9,\n               disable_progress_bar: bool = False) -> pd.DataFrame:\n    \"\"\"\n    Split a document into sentences. Can be used with `sectionize_documents`\n    to further split documents into more manageable pieces. Takes in offsets\n    to ensure that after splitting, the sentences can be matched to the\n    location in the original documents.\n\n    :param documents: Iterable containing documents which are strings\n    :param document_ids: Iterable containing document unique identifiers\n    :param offsets: Iterable tuple of the start and end indices\n    :param filter_len: Minimum character length of a sentence (otherwise filter out)\n    :return: Pandas DataFrame containing the columns `document_id`, `text`, `section`, `offset`\n    \"\"\"\n\n    document_sentences = []\n    for document, document_id, offset in tqdm(zip(documents, document_ids, offsets), total=len(documents), disable=disable_progress_bar):\n        try:\n            _, sentence_offsets = bf.text_to_sentences_and_offsets(document)\n            for o in sentence_offsets:\n                if o[1]-o[0] > filter_len:\n                    sentence = document[o[0]:o[1]]\n                    abs_offsets = (o[0]+offset[0], o[1]+offset[0])\n                    row = {}\n                    row['document_id'] = document_id\n                    row['text'] = sentence\n                    row['offset'] = abs_offsets\n                    document_sentences.append(row)\n        except:\n            continue\n    return pd.DataFrame(document_sentences)","metadata":{"execution":{"iopub.status.busy":"2023-09-23T19:05:25.447885Z","iopub.execute_input":"2023-09-23T19:05:25.448239Z","iopub.status.idle":"2023-09-23T19:05:25.457861Z","shell.execute_reply.started":"2023-09-23T19:05:25.448211Z","shell.execute_reply":"2023-09-23T19:05:25.456865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stop_words = ['each', 'you', 'the', 'use', 'used',\n                  'where', 'themselves', 'nor', \"it's\", 'how', \"don't\", 'just', 'your',\n                  'about', 'himself', 'with', \"weren't\", 'hers', \"wouldn't\", 'more', 'its', 'were',\n                  'his', 'their', 'then', 'been', 'myself', 're', 'not',\n                  'ours', 'will', 'needn', 'which', 'here', 'hadn', 'it', 'our', 'there', 'than',\n                  'most', \"couldn't\", 'both', 'some', 'for', 'up', 'couldn', \"that'll\",\n                  \"she's\", 'over', 'this', 'now', 'until', 'these', 'few', 'haven',\n                  'of', 'wouldn', 'into', 'too', 'to', 'very', 'shan', 'before', 'the', 'they',\n                  'between', \"doesn't\", 'are', 'was', 'out', 'we', 'me',\n                  'after', 'has', \"isn't\", 'have', 'such', 'should', 'yourselves', 'or', 'during', 'herself',\n                  'doing', 'in', \"shouldn't\", \"won't\", 'when', 'do', 'through', 'she',\n                  'having', 'him', \"haven't\", 'against', 'itself', 'that',\n                  'did', 'theirs', 'can', 'those',\n                  'own', 'so', 'and', 'who', \"you've\", 'yourself', 'her', 'he', 'only',\n                  'what', 'ourselves', 'again', 'had', \"you'd\", 'is', 'other',\n                  'why', 'while', 'from', 'them', 'if', 'above', 'does', 'whom',\n                  'yours', 'but', 'being', \"wasn't\", 'be']\nfrom sklearn.feature_extraction import text\nstop_words2 = text.ENGLISH_STOP_WORDS\nstop_words = list(stop_words2.union(stop_words))\n\ndf_valid = pd.read_csv(\"/kaggle/input/kaggle-llm-science-exam/test.csv\")","metadata":{"execution":{"iopub.status.busy":"2023-09-23T19:05:26.960774Z","iopub.execute_input":"2023-09-23T19:05:26.961479Z","iopub.status.idle":"2023-09-23T19:05:26.994336Z","shell.execute_reply.started":"2023-09-23T19:05:26.961441Z","shell.execute_reply":"2023-09-23T19:05:26.993385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"retrieved_articles_parsed = get_relevant_documents_parsed(df_valid)\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2023-09-23T19:05:34.060622Z","iopub.execute_input":"2023-09-23T19:05:34.060982Z","iopub.status.idle":"2023-09-23T19:16:23.449934Z","shell.execute_reply.started":"2023-09-23T19:05:34.060953Z","shell.execute_reply":"2023-09-23T19:16:23.448731Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# retrieved_articles = get_relevant_documents(df_valid)\n# gc.collect()","metadata":{"execution":{"iopub.status.busy":"2023-09-23T19:16:23.45202Z","iopub.execute_input":"2023-09-23T19:16:23.452494Z","iopub.status.idle":"2023-09-23T19:28:50.550408Z","shell.execute_reply.started":"2023-09-23T19:16:23.452447Z","shell.execute_reply":"2023-09-23T19:28:50.549272Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile new-wiki-30-clusters-more_part1.py\n# new-wiki-30-clusters-more\nfrom __future__ import annotations\nimport os\nimport os\nimport gc\nimport pandas as pd\nimport numpy as np\nimport re\nfrom tqdm.auto import tqdm\nimport blingfire as bf\n\nfrom collections.abc import Iterable\nimport faiss\nfrom faiss import write_index, read_index\nfrom sentence_transformers import SentenceTransformer\nimport matplotlib.pyplot as plt\n\nimport torch\nimport ctypes\nlibc = ctypes.CDLL(\"libc.so.6\")\nfrom dataclasses import dataclass\nfrom typing import Optional, Union\nfrom datasets import Dataset\nfrom transformers import AutoTokenizer\nfrom transformers import AutoModelForMultipleChoice, TrainingArguments, Trainer\nfrom transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\nfrom torch.utils.data import DataLoader\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport numpy as np\nimport pandas as pd \nfrom datasets import load_dataset, load_from_disk\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport torch\nfrom transformers import LongformerTokenizer, LongformerForMultipleChoice\nimport transformers\nimport pandas as pd\nimport pickle\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport unicodedata\n\nimport os\n# all-paraphs-parsed-expanded\nSIM_MODEL = '/kaggle/input/new-sentencetransformer/evaluate'\nDEVICE = 0\nMAX_LENGTH = 384\nBATCH_SIZE = 32\nWIKI_PATH = \"/kaggle/input/wikipedia-20230701\"\nwiki_files = os.listdir(WIKI_PATH)\n\nmodel = SentenceTransformer(SIM_MODEL, device='cuda')\nmodel.max_seq_length = MAX_LENGTH\n\ntrn = pd.read_csv(\"/kaggle/input/kaggle-llm-science-exam/test.csv\").drop(columns = ['id'])\ntrn.head()\ntrn['answer_all'] = trn.apply(lambda x: \"\\n\".join([x['A'], x['B'], x['C'], x['D'], x['E']]), axis=1)\n\n## Search using the prompt and answers to guide the search\ntrn['prompt_answer_stem'] = trn['prompt'] + \"\\n\" +trn['prompt'] + \"\\n\" +trn['prompt'] + \"\\n\"+ trn['answer_all']\n\ntext_index = read_index(\"/kaggle/input/new-sentencetransformer/index_new-wiki-30_part1.index\")\n\nprompt_embeddings = model.encode(trn.prompt_answer_stem.values, batch_size=BATCH_SIZE, device=DEVICE, show_progress_bar=True, convert_to_tensor=True, normalize_embeddings=True)\nprompt_embeddings = prompt_embeddings.detach().cpu().numpy()\n_ = gc.collect()\n\nsearch_score, search_index = text_index.search(prompt_embeddings, 5)\n\ncohere_dataset_filtered = load_from_disk(\"/kaggle/working/new-wiki-30-clusters-more\").to_pandas()\n\ncontexts3 = []\nfor i, (scr, idx) in tqdm(enumerate(zip(search_score, search_index)), total=len(search_score)):\n    contexts3.append(' '.join(cohere_dataset_filtered.loc[idx,'text'].tolist()))\nnp.save('contexts3_part1.npy',contexts3) \nnp.save(\"p1_index.npy\",search_index)\nnp.save(\"p1_score.npy\",search_score)\ndel text_index\ndel prompt_embeddings\ndel search_score\ndel search_index\ndel cohere_dataset_filtered\n_ = gc.collect()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile new-wiki-30-clusters-more_part2.py\n# new-wiki-30-clusters-more\nfrom __future__ import annotations\nimport os\nimport os\nimport gc\nimport pandas as pd\nimport numpy as np\nimport re\nfrom tqdm.auto import tqdm\nimport blingfire as bf\n\nfrom collections.abc import Iterable\nimport faiss\nfrom faiss import write_index, read_index\nfrom sentence_transformers import SentenceTransformer\nimport matplotlib.pyplot as plt\n\nimport torch\nimport ctypes\nlibc = ctypes.CDLL(\"libc.so.6\")\nfrom dataclasses import dataclass\nfrom typing import Optional, Union\nfrom datasets import Dataset\nfrom transformers import AutoTokenizer\nfrom transformers import AutoModelForMultipleChoice, TrainingArguments, Trainer\nfrom transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\nfrom torch.utils.data import DataLoader\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport numpy as np\nimport pandas as pd \nfrom datasets import load_dataset, load_from_disk\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport torch\nfrom transformers import LongformerTokenizer, LongformerForMultipleChoice\nimport transformers\nimport pandas as pd\nimport pickle\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport unicodedata\n\nimport os\n# all-paraphs-parsed-expanded\nSIM_MODEL = '/kaggle/input/new-sentencetransformer/evaluate'\nDEVICE = 0\nMAX_LENGTH = 384\nBATCH_SIZE = 32\nWIKI_PATH = \"/kaggle/input/wikipedia-20230701\"\nwiki_files = os.listdir(WIKI_PATH)\n\nmodel = SentenceTransformer(SIM_MODEL, device='cuda')\nmodel.max_seq_length = MAX_LENGTH\n\ntrn = pd.read_csv(\"/kaggle/input/kaggle-llm-science-exam/test.csv\").drop(columns = ['id'])\ntrn.head()\ntrn['answer_all'] = trn.apply(lambda x: \"\\n\".join([x['A'], x['B'], x['C'], x['D'], x['E']]), axis=1)\n\n## Search using the prompt and answers to guide the search\ntrn['prompt_answer_stem'] = trn['prompt'] + \"\\n\" +trn['prompt'] + \"\\n\" +trn['prompt'] + \"\\n\"+ trn['answer_all']\n\ntext_index = read_index(\"/kaggle/input/new-sentencetransformer/index_new-wiki-30_part2.index\")\n\nprompt_embeddings = model.encode(trn.prompt_answer_stem.values, batch_size=BATCH_SIZE, device=DEVICE, show_progress_bar=True, convert_to_tensor=True, normalize_embeddings=True)\nprompt_embeddings = prompt_embeddings.detach().cpu().numpy()\n_ = gc.collect()\n\nsearch_score, search_index = text_index.search(prompt_embeddings, 5)\n\ncohere_dataset_filtered = load_from_disk(\"/kaggle/working/new-wiki-30-clusters-more\").to_pandas()\n\ncontexts3 = []\nfor i, (scr, idx) in tqdm(enumerate(zip(search_score, search_index)), total=len(search_score)):\n    contexts3.append(' '.join(cohere_dataset_filtered.loc[idx,'text'].tolist()))\nnp.save('contexts3_part2.npy',contexts3) \nnp.save(\"p2_index.npy\",search_index)\nnp.save(\"p2_score.npy\",search_score)\ndel text_index\ndel prompt_embeddings\ndel search_score\ndel search_index\ndel cohere_dataset_filtered\n_ = gc.collect()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile new-wiki-30-clusters-more_part3.py\n# new-wiki-30-clusters-more\nfrom __future__ import annotations\nimport os\nimport os\nimport gc\nimport pandas as pd\nimport numpy as np\nimport re\nfrom tqdm.auto import tqdm\nimport blingfire as bf\n\nfrom collections.abc import Iterable\nimport faiss\nfrom faiss import write_index, read_index\nfrom sentence_transformers import SentenceTransformer\nimport matplotlib.pyplot as plt\n\nimport torch\nimport ctypes\nlibc = ctypes.CDLL(\"libc.so.6\")\nfrom dataclasses import dataclass\nfrom typing import Optional, Union\nfrom datasets import Dataset\nfrom transformers import AutoTokenizer\nfrom transformers import AutoModelForMultipleChoice, TrainingArguments, Trainer\nfrom transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\nfrom torch.utils.data import DataLoader\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport numpy as np\nimport pandas as pd \nfrom datasets import load_dataset, load_from_disk\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport torch\nfrom transformers import LongformerTokenizer, LongformerForMultipleChoice\nimport transformers\nimport pandas as pd\nimport pickle\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport unicodedata\n\nimport os\n# all-paraphs-parsed-expanded\nSIM_MODEL = '/kaggle/input/new-sentencetransformer/evaluate'\nDEVICE = 0\nMAX_LENGTH = 384\nBATCH_SIZE = 32\nWIKI_PATH = \"/kaggle/input/wikipedia-20230701\"\nwiki_files = os.listdir(WIKI_PATH)\n\nmodel = SentenceTransformer(SIM_MODEL, device='cuda')\nmodel.max_seq_length = MAX_LENGTH\n\ntrn = pd.read_csv(\"/kaggle/input/kaggle-llm-science-exam/test.csv\").drop(columns = ['id'])\ntrn.head()\ntrn['answer_all'] = trn.apply(lambda x: \"\\n\".join([x['A'], x['B'], x['C'], x['D'], x['E']]), axis=1)\n\n## Search using the prompt and answers to guide the search\ntrn['prompt_answer_stem'] = trn['prompt'] + \"\\n\" +trn['prompt'] + \"\\n\" +trn['prompt'] + \"\\n\"+ trn['answer_all']\n\ntext_index = read_index(\"/kaggle/input/new-sentencetransformer/index_new-wiki-30_part3.index\")\n\nprompt_embeddings = model.encode(trn.prompt_answer_stem.values, batch_size=BATCH_SIZE, device=DEVICE, show_progress_bar=True, convert_to_tensor=True, normalize_embeddings=True)\nprompt_embeddings = prompt_embeddings.detach().cpu().numpy()\n_ = gc.collect()\n\nsearch_score, search_index = text_index.search(prompt_embeddings, 5)\n\ncohere_dataset_filtered = load_from_disk(\"/kaggle/working/new-wiki-30-clusters-more\").to_pandas()\n\ncontexts3 = []\nfor i, (scr, idx) in tqdm(enumerate(zip(search_score, search_index)), total=len(search_score)):\n    contexts3.append(' '.join(cohere_dataset_filtered.loc[idx,'text'].tolist()))\nnp.save('contexts3_part3.npy',contexts3) \nnp.save(\"p3_index.npy\",search_index)\nnp.save(\"p3_score.npy\",search_score)\ndel text_index\ndel prompt_embeddings\ndel search_score\ndel search_index\ndel cohere_dataset_filtered\n_ = gc.collect()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile new-wiki-30-clusters-more_part4.py\n# new-wiki-30-clusters-more\nfrom __future__ import annotations\nimport os\nimport os\nimport gc\nimport pandas as pd\nimport numpy as np\nimport re\nfrom tqdm.auto import tqdm\nimport blingfire as bf\n\nfrom collections.abc import Iterable\nimport faiss\nfrom faiss import write_index, read_index\nfrom sentence_transformers import SentenceTransformer\nimport matplotlib.pyplot as plt\n\nimport torch\nimport ctypes\nlibc = ctypes.CDLL(\"libc.so.6\")\nfrom dataclasses import dataclass\nfrom typing import Optional, Union\nfrom datasets import Dataset\nfrom transformers import AutoTokenizer\nfrom transformers import AutoModelForMultipleChoice, TrainingArguments, Trainer\nfrom transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\nfrom torch.utils.data import DataLoader\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport numpy as np\nimport pandas as pd \nfrom datasets import load_dataset, load_from_disk\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport torch\nfrom transformers import LongformerTokenizer, LongformerForMultipleChoice\nimport transformers\nimport pandas as pd\nimport pickle\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport unicodedata\n\nimport os\n# all-paraphs-parsed-expanded\nSIM_MODEL = '/kaggle/input/new-sentencetransformer/evaluate'\nDEVICE = 0\nMAX_LENGTH = 384\nBATCH_SIZE = 32\nWIKI_PATH = \"/kaggle/input/wikipedia-20230701\"\nwiki_files = os.listdir(WIKI_PATH)\n\nmodel = SentenceTransformer(SIM_MODEL, device='cuda')\nmodel.max_seq_length = MAX_LENGTH\n\ntrn = pd.read_csv(\"/kaggle/input/kaggle-llm-science-exam/test.csv\").drop(columns = ['id'])\ntrn.head()\ntrn['answer_all'] = trn.apply(lambda x: \"\\n\".join([x['A'], x['B'], x['C'], x['D'], x['E']]), axis=1)\n\n## Search using the prompt and answers to guide the search\ntrn['prompt_answer_stem'] = trn['prompt'] + \"\\n\" +trn['prompt'] + \"\\n\" +trn['prompt'] + \"\\n\"+ trn['answer_all']\n\ntext_index = read_index(\"/kaggle/input/new-sentencetransformer/index_new-wiki-30_part4.index\")\n\nprompt_embeddings = model.encode(trn.prompt_answer_stem.values, batch_size=BATCH_SIZE, device=DEVICE, show_progress_bar=True, convert_to_tensor=True, normalize_embeddings=True)\nprompt_embeddings = prompt_embeddings.detach().cpu().numpy()\n_ = gc.collect()\n\nsearch_score, search_index = text_index.search(prompt_embeddings, 5)\n\ncohere_dataset_filtered = load_from_disk(\"/kaggle/working/new-wiki-30-clusters-more\").to_pandas()\n\ncontexts3 = []\nfor i, (scr, idx) in tqdm(enumerate(zip(search_score, search_index)), total=len(search_score)):\n    contexts3.append(' '.join(cohere_dataset_filtered.loc[idx,'text'].tolist()))\nnp.save('contexts3_part4.npy',contexts3) \nnp.save(\"p4_index.npy\",search_index)\nnp.save(\"p4_score.npy\",search_score)\ndel text_index\ndel prompt_embeddings\ndel search_score\ndel search_index\ndel cohere_dataset_filtered\n_ = gc.collect()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile new-wiki-30-clusters-more_part5.py\n# new-wiki-30-clusters-more\nfrom __future__ import annotations\nimport os\nimport os\nimport gc\nimport pandas as pd\nimport numpy as np\nimport re\nfrom tqdm.auto import tqdm\nimport blingfire as bf\n\nfrom collections.abc import Iterable\nimport faiss\nfrom faiss import write_index, read_index\nfrom sentence_transformers import SentenceTransformer\nimport matplotlib.pyplot as plt\n\nimport torch\nimport ctypes\nlibc = ctypes.CDLL(\"libc.so.6\")\nfrom dataclasses import dataclass\nfrom typing import Optional, Union\nfrom datasets import Dataset\nfrom transformers import AutoTokenizer\nfrom transformers import AutoModelForMultipleChoice, TrainingArguments, Trainer\nfrom transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\nfrom torch.utils.data import DataLoader\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport numpy as np\nimport pandas as pd \nfrom datasets import load_dataset, load_from_disk\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport torch\nfrom transformers import LongformerTokenizer, LongformerForMultipleChoice\nimport transformers\nimport pandas as pd\nimport pickle\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport unicodedata\n\nimport os\n# all-paraphs-parsed-expanded\nSIM_MODEL = '/kaggle/input/new-sentencetransformer/evaluate'\nDEVICE = 0\nMAX_LENGTH = 384\nBATCH_SIZE = 32\nWIKI_PATH = \"/kaggle/input/wikipedia-20230701\"\nwiki_files = os.listdir(WIKI_PATH)\n\nmodel = SentenceTransformer(SIM_MODEL, device='cuda')\nmodel.max_seq_length = MAX_LENGTH\n\ntrn = pd.read_csv(\"/kaggle/input/kaggle-llm-science-exam/test.csv\").drop(columns = ['id'])\ntrn.head()\ntrn['answer_all'] = trn.apply(lambda x: \"\\n\".join([x['A'], x['B'], x['C'], x['D'], x['E']]), axis=1)\n\n## Search using the prompt and answers to guide the search\ntrn['prompt_answer_stem'] = trn['prompt'] + \"\\n\" +trn['prompt'] + \"\\n\" +trn['prompt'] + \"\\n\"+ trn['answer_all']\n\ntext_index = read_index(\"/kaggle/input/new-sentencetransformer/index_new-wiki-30_part5.index\")\n\nprompt_embeddings = model.encode(trn.prompt_answer_stem.values, batch_size=BATCH_SIZE, device=DEVICE, show_progress_bar=True, convert_to_tensor=True, normalize_embeddings=True)\nprompt_embeddings = prompt_embeddings.detach().cpu().numpy()\n_ = gc.collect()\n\nsearch_score, search_index = text_index.search(prompt_embeddings, 5)\n\ncohere_dataset_filtered = load_from_disk(\"/kaggle/working/new-wiki-30-clusters-more\").to_pandas()\n\ncontexts3 = []\nfor i, (scr, idx) in tqdm(enumerate(zip(search_score, search_index)), total=len(search_score)):\n    contexts3.append(' '.join(cohere_dataset_filtered.loc[idx,'text'].tolist()))\nnp.save('contexts3_part5.npy',contexts3) \nnp.save(\"p5_index.npy\",search_index)\nnp.save(\"p5_score.npy\",search_score)\ndel text_index\ndel prompt_embeddings\ndel search_score\ndel search_index\ndel cohere_dataset_filtered\n_ = gc.collect()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python3 new-wiki-30-clusters-more_part1.py","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python3 new-wiki-30-clusters-more_part2.py","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python3 new-wiki-30-clusters-more_part3.py","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python3 new-wiki-30-clusters-more_part4.py","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python3 new-wiki-30-clusters-more_part5.py","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile get_context3.py\nfrom __future__ import annotations\nimport os\nimport os\nimport gc\nimport pandas as pd\nimport numpy as np\nimport re\nfrom tqdm.auto import tqdm\nimport blingfire as bf\n\nfrom collections.abc import Iterable\nimport faiss\nfrom faiss import write_index, read_index\nfrom sentence_transformers import SentenceTransformer\nimport matplotlib.pyplot as plt\n\nimport torch\nimport ctypes\nlibc = ctypes.CDLL(\"libc.so.6\")\nfrom dataclasses import dataclass\nfrom typing import Optional, Union\nfrom datasets import Dataset\nfrom transformers import AutoTokenizer\nfrom transformers import AutoModelForMultipleChoice, TrainingArguments, Trainer\nfrom transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\nfrom torch.utils.data import DataLoader\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport numpy as np\nimport pandas as pd \nfrom datasets import load_dataset, load_from_disk\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport torch\nfrom transformers import LongformerTokenizer, LongformerForMultipleChoice\nimport transformers\nimport pandas as pd\nimport pickle\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport unicodedata\n\nimport os\n\ncontexts = []\nscores = []\nindexs = []\nindex1 = np.load('/kaggle/working/p1_index.npy').tolist()\nindex2 = np.load('/kaggle/working/p2_index.npy').tolist()\nindex3 = np.load('/kaggle/working/p3_index.npy').tolist()\nindex4 = np.load('/kaggle/working/p4_index.npy').tolist()\nindex5 = np.load('/kaggle/working/p5_index.npy').tolist()\n\nscore1 = np.load(\"/kaggle/working/p1_score.npy\").tolist()\nscore2 = np.load(\"/kaggle/working/p2_score.npy\").tolist()\nscore3 = np.load(\"/kaggle/working/p3_score.npy\").tolist()\nscore4 = np.load(\"/kaggle/working/p4_score.npy\").tolist()\nscore5 = np.load(\"/kaggle/working/p5_score.npy\").tolist()\ncontexts = []\nscores = []\nindexs = []\nfor i in range(0,len(index1)):\n    indexs.append(index1[i] + index2[i] + index3[i] + index4[i] +index5[i])\n    scores.append(score1[i] + score2[i] + score3[i] + score4[i] +score5[i])\n\nsorted_scores = []\nsorted_indexs = []\n\nfor s, i in zip(scores, indexs):\n    sorted_data = sorted(zip(s, i), key=lambda x: x[0],reverse = True)\n    sorted_s, sorted_i = zip(*sorted_data)\n    sorted_scores.append(list(sorted_s))\n    sorted_indexs.append(list(sorted_i))\n\ncohere_dataset_filtered = load_from_disk(\"/kaggle/input/new-wiki-30-clusters-more\").to_pandas()\ncontexts3 = []\nfor i, (scr, idx) in tqdm(enumerate(zip(sorted_scores, sorted_indexs)), total=len(sorted_scores)):\n    contexts3.append(' '.join(cohere_dataset_filtered.loc[idx,'text'].tolist()))\n\nnp.save('contexts3.npy',contexts3)\ndel cohere_dataset_filtered","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python3 get_context3.py","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# %%writefile all-paraphs-parsed-expanded.py\n# from __future__ import annotations\n# import os\n# import os\n# import gc\n# import pandas as pd\n# import numpy as np\n# import re\n# from tqdm.auto import tqdm\n# import blingfire as bf\n\n# from collections.abc import Iterable\n# import faiss\n# from faiss import write_index, read_index\n# from sentence_transformers import SentenceTransformer\n# import matplotlib.pyplot as plt\n\n# import torch\n# import ctypes\n# libc = ctypes.CDLL(\"libc.so.6\")\n# from dataclasses import dataclass\n# from typing import Optional, Union\n# from datasets import Dataset\n# from transformers import AutoTokenizer\n# from transformers import AutoModelForMultipleChoice, TrainingArguments, Trainer\n# from transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\n# from torch.utils.data import DataLoader\n# import warnings\n# warnings.filterwarnings(\"ignore\")\n\n# import numpy as np\n# import pandas as pd \n# from datasets import load_dataset, load_from_disk\n# from sklearn.feature_extraction.text import TfidfVectorizer\n# import torch\n# from transformers import LongformerTokenizer, LongformerForMultipleChoice\n# import transformers\n# import pandas as pd\n# import pickle\n# import numpy as np\n# import matplotlib.pyplot as plt\n# from tqdm import tqdm\n# import unicodedata\n\n# import os\n# # all-paraphs-parsed-expanded\n# SIM_MODEL = '/kaggle/input/new-sentencetransformer/evaluate'\n# DEVICE = 0\n# MAX_LENGTH = 384\n# BATCH_SIZE = 32\n# WIKI_PATH = \"/kaggle/input/wikipedia-20230701\"\n# wiki_files = os.listdir(WIKI_PATH)\n\n# model = SentenceTransformer(SIM_MODEL, device='cuda')\n# model.max_seq_length = MAX_LENGTH\n\n# trn = pd.read_csv(\"/kaggle/input/kaggle-llm-science-exam/test.csv\").drop(columns = ['id'])\n# trn.head()\n# trn['answer_all'] = trn.apply(lambda x: \"\\n\".join([x['A'], x['B'], x['C'], x['D'], x['E']]), axis=1)\n\n# ## Search using the prompt and answers to guide the search\n# trn['prompt_answer_stem'] = trn['prompt'] + \"\\n\" +trn['prompt'] + \"\\n\" +trn['prompt'] + \"\\n\"+ trn['answer_all']\n\n# text_index = read_index(\"/kaggle/input/all-pharse-new-gte/allV2.index\")\n\n# prompt_embeddings = model.encode(trn.prompt_answer_stem.values, batch_size=BATCH_SIZE, device=DEVICE, show_progress_bar=True, convert_to_tensor=True, normalize_embeddings=True)\n# prompt_embeddings = prompt_embeddings.detach().cpu().numpy()\n# _ = gc.collect()\n\n# search_score, search_index = text_index.search(prompt_embeddings, 5)\n\n# cohere_dataset_filtered = load_from_disk(\"/kaggle/input/all-paraphs-parsed-expanded\").to_pandas()\n\n# contexts4 = []\n# for i, (scr, idx) in tqdm(enumerate(zip(search_score, search_index)), total=len(search_score)):\n#     contexts4.append(' '.join(cohere_dataset_filtered.loc[idx,'text'].tolist()))\n    \n# np.save('contexts4.npy',contexts4)\n\n# del text_index\n# del prompt_embeddings\n# del search_score\n# del search_index\n# del cohere_dataset_filtered","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !python3 /kaggle/working/all-paraphs-parsed-expanded.py","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# context4 = np.load('/kaggle/working/contexts4.npy')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"context3 = np.load('/kaggle/working/contexts3.npy')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SIM_MODEL = '/kaggle/input/sentence-transformer-model/gte-small'\nDEVICE = 0\nMAX_LENGTH = 384\nBATCH_SIZE = 32\nWIKI_PATH = \"/kaggle/input/wikipedia-20230701\"\nwiki_files = os.listdir(WIKI_PATH)\n\ntrn = pd.read_csv(\"/kaggle/input/kaggle-llm-science-exam/test.csv\").drop(columns = ['id'])\ntrn.head()\n## Combine all answers\ntrn['answer_all'] = trn.apply(lambda x: \" \".join([x['A'], x['B'], x['C'], x['D'], x['E']]), axis=1)\n\n## Search using the prompt and answers to guide the search\ntrn['prompt_answer_stem'] = trn['prompt'] + \" \" + trn['answer_all']\n\nmodel = SentenceTransformer(SIM_MODEL, device='cuda')\nmodel.max_seq_length = MAX_LENGTH\n# model = model.half()\n\nsentence_index = read_index(\"/kaggle/input/d/lizhecheng/wikipedia-faiss-index/merged.index\")\n\nprompt_embeddings = model.encode(trn.prompt_answer_stem.values, batch_size=BATCH_SIZE, device=DEVICE, show_progress_bar=True, convert_to_tensor=True, normalize_embeddings=True)\nprompt_embeddings = prompt_embeddings.detach().cpu().numpy()\n_ = gc.collect()\n\nsearch_score, search_index = sentence_index.search(prompt_embeddings, 10)\n\ndel sentence_index\ndel prompt_embeddings\n_ = gc.collect()\nlibc.malloc_trim(0)\n\ndf = pd.read_parquet(\"/kaggle/input/wikipedia-20230701/wiki_2023_index.parquet\",\n                     columns=['id', 'file'])\n\nwikipedia_file_data = []\n\nfor i, (scr, idx) in tqdm(enumerate(zip(search_score, search_index)), total=len(search_score)):\n    scr_idx = idx\n    _df = df.loc[scr_idx].copy()\n    _df['prompt_id'] = i\n    wikipedia_file_data.append(_df)\nwikipedia_file_data = pd.concat(wikipedia_file_data).reset_index(drop=True)\nwikipedia_file_data = wikipedia_file_data[['id', 'prompt_id', 'file']].drop_duplicates().sort_values(['file', 'id']).reset_index(drop=True)\n\n## Save memory - delete df since it is no longer necessary\ndel df\n_ = gc.collect()\nlibc.malloc_trim(0)\n\nwiki_text_data = []\n\nfor file in tqdm(wikipedia_file_data.file.unique(), total=len(wikipedia_file_data.file.unique())):\n    _id = [str(i) for i in wikipedia_file_data[wikipedia_file_data['file']==file]['id'].tolist()]\n    _df = pd.read_parquet(f\"{WIKI_PATH}/{file}\", columns=['id', 'text'])\n\n    _df_temp = _df[_df['id'].isin(_id)].copy()\n    del _df\n    _ = gc.collect()\n    libc.malloc_trim(0)\n    wiki_text_data.append(_df_temp)\nwiki_text_data = pd.concat(wiki_text_data).drop_duplicates().reset_index(drop=True)\n_ = gc.collect()\n\nprocessed_wiki_text_data = process_documents(wiki_text_data.text.values, wiki_text_data.id.values)\n\nwiki_data_embeddings = model.encode(processed_wiki_text_data.text,\n                                    batch_size=BATCH_SIZE,\n                                    device=DEVICE,\n                                    show_progress_bar=True,\n                                    convert_to_tensor=True,\n                                    normalize_embeddings=True)#.half()\nwiki_data_embeddings = wiki_data_embeddings.detach().cpu().numpy()\n\n_ = gc.collect()\n\nquestion_embeddings = model.encode(trn.prompt_answer_stem.values, batch_size=BATCH_SIZE, device=DEVICE, show_progress_bar=True, convert_to_tensor=True, normalize_embeddings=True)\nquestion_embeddings = question_embeddings.detach().cpu().numpy()\n\n## Parameter to determine how many relevant sentences to include\nNUM_SENTENCES_INCLUDE = 30\n\n## List containing just Context\ncontexts = []\nfor r in tqdm(trn.itertuples(), total=len(trn)):\n\n    prompt_id = r.Index\n\n    prompt_indices = processed_wiki_text_data[processed_wiki_text_data['document_id'].isin(wikipedia_file_data[wikipedia_file_data['prompt_id']==prompt_id]['id'].values)].index.values\n\n    if prompt_indices.shape[0] > 0:\n        prompt_index = faiss.index_factory(wiki_data_embeddings.shape[1], \"Flat\")\n        prompt_index.add(wiki_data_embeddings[prompt_indices])\n\n        context = \"\"\n        \n        ## Get the top matches\n        ss, ii = prompt_index.search(question_embeddings, NUM_SENTENCES_INCLUDE)\n        for _s, _i in zip(ss[prompt_id], ii[prompt_id]):\n            context += processed_wiki_text_data.loc[prompt_indices]['text'].iloc[_i] + ' '\n        \n    contexts.append(context)\n    \ntrn['context'] = contexts\n\ntrn[[\"prompt\", \"context\", \"A\", \"B\", \"C\", \"D\", \"E\"]].to_csv(\"./test_context.csv\", index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df = pd.read_csv(\"test_context.csv\")\ntest_df.index = list(range(len(test_df)))\ntest_df['id'] = list(range(len(test_df)))\ntest_df[\"prompt\"] = test_df[\"context\"].apply(lambda x: x[:2300]) + \" #### \" +  test_df[\"prompt\"]\ntest_df['answer'] = 'A'","metadata":{"execution":{"iopub.status.busy":"2023-09-13T13:08:27.282465Z","iopub.status.idle":"2023-09-13T13:08:27.283209Z","shell.execute_reply.started":"2023-09-13T13:08:27.282967Z","shell.execute_reply":"2023-09-13T13:08:27.28299Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test_df2 = pd.read_csv(\"test_context.csv\")\n# test_df2.index = list(range(len(test_df2)))\n# test_df2['id'] = list(range(len(test_df2)))\n# test_df2[\"prompt\"] = test_df2[\"context\"].apply(lambda x: x[:1750]) + \" #### \" +  test_df2[\"prompt\"]  ### This is for 384 model\n# test_df2['answer'] = 'A'\n","metadata":{"execution":{"iopub.status.busy":"2023-09-13T13:08:27.284562Z","iopub.status.idle":"2023-09-13T13:08:27.285354Z","shell.execute_reply.started":"2023-09-13T13:08:27.285093Z","shell.execute_reply":"2023-09-13T13:08:27.285117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df3 = pd.read_csv(\"test_context.csv\")\ntest_df3.index = list(range(len(test_df3)))\ntest_df3['id'] = list(range(len(test_df3)))\ntest_df3['answer'] = 'A'\ntest_df3[\"context\"] = context3\n# for index in tqdm(range(test_df3.shape[0])):\n#     context1 = '\\n'.join([t for _, _, t in retrieved_articles[index][::-1]])\n# #     context1 = f\"{retrieved_articles[index][-4][2]}\\n{retrieved_articles[index][-3][2]}\\n{retrieved_articles[index][-2][2]}\\n{retrieved_articles[index][-1][2]}\"\n#     test_df3.loc[index,'context'] = context1\ntest_df3[\"prompt\"] = test_df3[\"context\"].apply(lambda x: x[:2300]) + \" #### \" +  test_df3[\"prompt\"]  ### This is for 384 model","metadata":{"execution":{"iopub.status.busy":"2023-09-23T19:42:31.573148Z","iopub.execute_input":"2023-09-23T19:42:31.573734Z","iopub.status.idle":"2023-09-23T19:42:31.634794Z","shell.execute_reply.started":"2023-09-23T19:42:31.573681Z","shell.execute_reply":"2023-09-23T19:42:31.633663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df4 = pd.read_csv(\"test_context.csv\")\ntest_df4.index = list(range(len(test_df4)))\ntest_df4['id'] = list(range(len(test_df4)))\ntest_df4['answer'] = 'A'\n# test_df4[\"context\"] = context4\nfor index in tqdm(range(test_df4.shape[0])):\n    context2 = '\\n'.join([t for _, _, t in retrieved_articles_parsed[index][::-1]])\n#     context2 = f\"{retrieved_articles_parsed[index][-3][2]}\\n{retrieved_articles_parsed[index][-2][2]}\\n{retrieved_articles_parsed[index][-1][2]}\"\n    test_df4.loc[index,'context'] = context2\ntest_df4[\"prompt\"] = test_df4[\"context\"].apply(lambda x: x[:2300]) + \" #### \" +  test_df4[\"prompt\"]  ### This is for 384 model","metadata":{"execution":{"iopub.status.busy":"2023-09-23T19:43:19.853216Z","iopub.execute_input":"2023-09-23T19:43:19.854233Z","iopub.status.idle":"2023-09-23T19:43:19.913597Z","shell.execute_reply.started":"2023-09-23T19:43:19.854195Z","shell.execute_reply":"2023-09-23T19:43:19.912607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_dir = \"/kaggle/input/222k-133k-finetuned-deberta-v3-models/133k fold5/checkpoint-7100\"\ntokenizer = AutoTokenizer.from_pretrained(model_dir)\nmodel = AutoModelForMultipleChoice.from_pretrained(model_dir).cuda()\nmodel.eval()","metadata":{"execution":{"iopub.status.busy":"2023-09-23T19:41:47.219406Z","iopub.execute_input":"2023-09-23T19:41:47.219877Z","iopub.status.idle":"2023-09-23T19:42:07.164256Z","shell.execute_reply.started":"2023-09-23T19:41:47.219839Z","shell.execute_reply":"2023-09-23T19:42:07.162477Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We'll create a dictionary to convert option names (A, B, C, D, E) into indices and back again\noptions = 'ABCDE'\nindices = list(range(5))\n\noption_to_index = {option: index for option, index in zip(options, indices)}\nindex_to_option = {index: option for option, index in zip(options, indices)}\n\ndef preprocess(example):\n    # The AutoModelForMultipleChoice class expects a set of question/answer pairs\n    # so we'll copy our question 5 times before tokenizing\n    first_sentence = [example['prompt']] * 5\n    second_sentence = []\n    for option in options:\n        second_sentence.append(example[option])\n    # Our tokenizer will turn our text into token IDs BERT can understand\n    tokenized_example = tokenizer(first_sentence, second_sentence, truncation=True)\n    tokenized_example['label'] = option_to_index[example['answer']]\n    return tokenized_example","metadata":{"execution":{"iopub.status.busy":"2023-09-23T19:43:26.34301Z","iopub.execute_input":"2023-09-23T19:43:26.3434Z","iopub.status.idle":"2023-09-23T19:43:26.352275Z","shell.execute_reply.started":"2023-09-23T19:43:26.343361Z","shell.execute_reply":"2023-09-23T19:43:26.351171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@dataclass\nclass DataCollatorForMultipleChoice:\n    tokenizer: PreTrainedTokenizerBase\n    padding: Union[bool, str, PaddingStrategy] = True\n    max_length: Optional[int] = None\n    pad_to_multiple_of: Optional[int] = None\n    \n    def __call__(self, features):\n        label_name = \"label\" if 'label' in features[0].keys() else 'labels'\n        labels = [feature.pop(label_name) for feature in features]\n        batch_size = len(features)\n        num_choices = len(features[0]['input_ids'])\n        flattened_features = [\n            [{k: v[i] for k, v in feature.items()} for i in range(num_choices)] for feature in features\n        ]\n        flattened_features = sum(flattened_features, [])\n        \n        batch = self.tokenizer.pad(\n            flattened_features,\n            padding=self.padding,\n            max_length=self.max_length,\n            pad_to_multiple_of=self.pad_to_multiple_of,\n            return_tensors='pt',\n        )\n        batch = {k: v.view(batch_size, num_choices, -1) for k, v in batch.items()}\n        batch['labels'] = torch.tensor(labels, dtype=torch.int64)\n        return batch","metadata":{"execution":{"iopub.status.busy":"2023-09-23T19:43:27.585787Z","iopub.execute_input":"2023-09-23T19:43:27.586676Z","iopub.status.idle":"2023-09-23T19:43:27.597112Z","shell.execute_reply.started":"2023-09-23T19:43:27.58663Z","shell.execute_reply":"2023-09-23T19:43:27.596072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenized_test_dataset = Dataset.from_pandas(test_df[['id', 'prompt', 'A', 'B', 'C', 'D', 'E', 'answer']].drop(columns=['id'])).map(preprocess, remove_columns=['prompt', 'A', 'B', 'C', 'D', 'E', 'answer'])\ntokenized_test_dataset = tokenized_test_dataset.remove_columns([\"__index_level_0__\"])\ndata_collator = DataCollatorForMultipleChoice(tokenizer=tokenizer)\ntest_dataloader = DataLoader(tokenized_test_dataset, batch_size=1, shuffle=False, collate_fn=data_collator)","metadata":{"execution":{"iopub.status.busy":"2023-09-13T13:08:27.293401Z","iopub.status.idle":"2023-09-13T13:08:27.294196Z","shell.execute_reply.started":"2023-09-13T13:08:27.293943Z","shell.execute_reply":"2023-09-13T13:08:27.293969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# tokenized_test_dataset2 = Dataset.from_pandas(test_df2[['id', 'prompt', 'A', 'B', 'C', 'D', 'E', 'answer']].drop(columns=['id'])).map(preprocess, remove_columns=['prompt', 'A', 'B', 'C', 'D', 'E', 'answer'])\n# tokenized_test_dataset2 = tokenized_test_dataset2.remove_columns([\"__index_level_0__\"])\n# data_collator2 = DataCollatorForMultipleChoice(tokenizer=tokenizer)\n# test_dataloader2 = DataLoader(tokenized_test_dataset2, batch_size=1, shuffle=False, collate_fn=data_collator2)","metadata":{"execution":{"iopub.status.busy":"2023-09-13T13:08:27.295584Z","iopub.status.idle":"2023-09-13T13:08:27.296381Z","shell.execute_reply.started":"2023-09-13T13:08:27.296118Z","shell.execute_reply":"2023-09-13T13:08:27.296142Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenized_test_dataset3 = Dataset.from_pandas(test_df3[['id', 'prompt', 'A', 'B', 'C', 'D', 'E', 'answer']].drop(columns=['id'])).map(preprocess, remove_columns=['prompt', 'A', 'B', 'C', 'D', 'E', 'answer'])\ntokenized_test_dataset3 = tokenized_test_dataset3.remove_columns([\"__index_level_0__\"])\ndata_collator3 = DataCollatorForMultipleChoice(tokenizer=tokenizer)\ntest_dataloader3 = DataLoader(tokenized_test_dataset3, batch_size=1, shuffle=False, collate_fn=data_collator3)","metadata":{"execution":{"iopub.status.busy":"2023-09-23T19:44:51.858207Z","iopub.execute_input":"2023-09-23T19:44:51.859201Z","iopub.status.idle":"2023-09-23T19:44:53.305294Z","shell.execute_reply.started":"2023-09-23T19:44:51.859164Z","shell.execute_reply":"2023-09-23T19:44:53.304316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenized_test_dataset4 = Dataset.from_pandas(test_df4[['id', 'prompt', 'A', 'B', 'C', 'D', 'E', 'answer']].drop(columns=['id'])).map(preprocess, remove_columns=['prompt', 'A', 'B', 'C', 'D', 'E', 'answer'])\ntokenized_test_dataset4 = tokenized_test_dataset4.remove_columns([\"__index_level_0__\"])\ndata_collator4 = DataCollatorForMultipleChoice(tokenizer=tokenizer)\ntest_dataloader4 = DataLoader(tokenized_test_dataset4, batch_size=1, shuffle=False, collate_fn=data_collator4)","metadata":{"execution":{"iopub.status.busy":"2023-09-23T19:45:13.298915Z","iopub.execute_input":"2023-09-23T19:45:13.299298Z","iopub.status.idle":"2023-09-23T19:45:15.017715Z","shell.execute_reply.started":"2023-09-23T19:45:13.299267Z","shell.execute_reply":"2023-09-23T19:45:15.016555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def softmax(x):\n    e_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n    return e_x / e_x.sum(axis=-1, keepdims = True)    ","metadata":{"execution":{"iopub.status.busy":"2023-09-23T19:45:33.927403Z","iopub.execute_input":"2023-09-23T19:45:33.928585Z","iopub.status.idle":"2023-09-23T19:45:33.93465Z","shell.execute_reply.started":"2023-09-23T19:45:33.92854Z","shell.execute_reply":"2023-09-23T19:45:33.933175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('start to predict')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\ntest_predictions = []\nfor batch in test_dataloader:\n    for k in batch.keys():\n        batch[k] = batch[k].cuda()\n    with torch.no_grad():\n        outputs = model(**batch)\n    test_predictions.append(outputs.logits.cpu().detach())\n\ntest_predictions = torch.cat(test_predictions)\ntest_predictions = test_predictions.numpy()\n\ntest_predictions2 = []\nfor batch in test_dataloader3:\n    for k in batch.keys():\n        batch[k] = batch[k].cuda()\n    with torch.no_grad():\n        outputs = model(**batch)\n    test_predictions2.append(outputs.logits.cpu().detach())\n\ntest_predictions2 = torch.cat(test_predictions2)\ntest_predictions2 = test_predictions2.numpy()\n\ntest_predictions3 = []\nfor batch in test_dataloader4:\n    for k in batch.keys():\n        batch[k] = batch[k].cuda()\n    with torch.no_grad():\n        outputs = model(**batch)\n    test_predictions3.append(outputs.logits.cpu().detach())\n\ntest_predictions3 = torch.cat(test_predictions3)\ntest_predictions3 = test_predictions3.numpy()","metadata":{"execution":{"iopub.status.busy":"2023-09-23T19:46:58.288196Z","iopub.execute_input":"2023-09-23T19:46:58.288608Z","iopub.status.idle":"2023-09-23T19:48:06.112899Z","shell.execute_reply.started":"2023-09-23T19:46:58.288576Z","shell.execute_reply":"2023-09-23T19:48:06.111854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nmodel_dir = \"/kaggle/input/final-50k/checkpoint-2800\"\ntokenizer = AutoTokenizer.from_pretrained(model_dir)\nmodel = AutoModelForMultipleChoice.from_pretrained(model_dir).cuda()\nmodel.eval()","metadata":{"execution":{"iopub.status.busy":"2023-09-13T13:08:27.306238Z","iopub.status.idle":"2023-09-13T13:08:27.307035Z","shell.execute_reply.started":"2023-09-13T13:08:27.306748Z","shell.execute_reply":"2023-09-13T13:08:27.306797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_predictions4 = []\nfor batch in test_dataloader:\n    for k in batch.keys():\n        batch[k] = batch[k].cuda()\n    with torch.no_grad():\n        outputs = model(**batch)\n    test_predictions4.append(outputs.logits.cpu().detach())\n\ntest_predictions4 = torch.cat(test_predictions4)\ntest_predictions4 = test_predictions4.numpy()\n\ntest_predictions5 = []\nfor batch in test_dataloader3:\n    for k in batch.keys():\n        batch[k] = batch[k].cuda()\n    with torch.no_grad():\n        outputs = model(**batch)\n    test_predictions5.append(outputs.logits.cpu().detach())\n\ntest_predictions5 = torch.cat(test_predictions5)\ntest_predictions5 = test_predictions5.numpy()\n\ntest_predictions6 = []\nfor batch in test_dataloader4:\n    for k in batch.keys():\n        batch[k] = batch[k].cuda()\n    with torch.no_grad():\n        outputs = model(**batch)\n    test_predictions6.append(outputs.logits.cpu().detach())\n\ntest_predictions6 = torch.cat(test_predictions6)\ntest_predictions6 = test_predictions6.numpy()","metadata":{"execution":{"iopub.status.busy":"2023-09-13T13:08:27.308671Z","iopub.status.idle":"2023-09-13T13:08:27.309454Z","shell.execute_reply.started":"2023-09-13T13:08:27.309206Z","shell.execute_reply":"2023-09-13T13:08:27.309238Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nmodel_dir = \"/kaggle/input/singlegpu-200k-run-sft-awp-0-1-ls-dropout-4e6/checkpoint-11300\"\ntokenizer = AutoTokenizer.from_pretrained(model_dir)\nmodel = AutoModelForMultipleChoice.from_pretrained(model_dir).cuda()\nmodel.eval()","metadata":{"execution":{"iopub.status.busy":"2023-09-13T13:08:27.310787Z","iopub.status.idle":"2023-09-13T13:08:27.311518Z","shell.execute_reply.started":"2023-09-13T13:08:27.311277Z","shell.execute_reply":"2023-09-13T13:08:27.311301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_predictions7 = []\nfor batch in test_dataloader:\n    for k in batch.keys():\n        batch[k] = batch[k].cuda()\n    with torch.no_grad():\n        outputs = model(**batch)\n    test_predictions7.append(outputs.logits.cpu().detach())\n\ntest_predictions7 = torch.cat(test_predictions7)\ntest_predictions7 = test_predictions7.numpy()\n\ntest_predictions8 = []\nfor batch in test_dataloader3:\n    for k in batch.keys():\n        batch[k] = batch[k].cuda()\n    with torch.no_grad():\n        outputs = model(**batch)\n    test_predictions8.append(outputs.logits.cpu().detach())\n\ntest_predictions8 = torch.cat(test_predictions8)\ntest_predictions8 = test_predictions8.numpy()\n\ntest_predictions9 = []\nfor batch in test_dataloader4:\n    for k in batch.keys():\n        batch[k] = batch[k].cuda()\n    with torch.no_grad():\n        outputs = model(**batch)\n    test_predictions9.append(outputs.logits.cpu().detach())\n\ntest_predictions9 = torch.cat(test_predictions9)\ntest_predictions9 = test_predictions9.numpy()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nmodel_dir = \"/kaggle/input/768-300k-best-para-epoch2/checkpoint-18900\"\ntokenizer = AutoTokenizer.from_pretrained(model_dir)\nmodel = AutoModelForMultipleChoice.from_pretrained(model_dir).cuda()\nmodel.eval()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_predictions10 = []\nfor batch in test_dataloader:\n    for k in batch.keys():\n        batch[k] = batch[k].cuda()\n    with torch.no_grad():\n        outputs = model(**batch)\n    test_predictions10.append(outputs.logits.cpu().detach())\n\ntest_predictions10 = torch.cat(test_predictions10)\ntest_predictions10 = test_predictions10.numpy()\n\ntest_predictions11 = []\nfor batch in test_dataloader3:\n    for k in batch.keys():\n        batch[k] = batch[k].cuda()\n    with torch.no_grad():\n        outputs = model(**batch)\n    test_predictions11.append(outputs.logits.cpu().detach())\n\ntest_predictions11 = torch.cat(test_predictions11)\ntest_predictions11 = test_predictions11.numpy()\n\ntest_predictions12 = []\nfor batch in test_dataloader4:\n    for k in batch.keys():\n        batch[k] = batch[k].cuda()\n    with torch.no_grad():\n        outputs = model(**batch)\n    test_predictions12.append(outputs.logits.cpu().detach())\n\ntest_predictions12 = torch.cat(test_predictions12)\ntest_predictions12 = test_predictions12.numpy()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"weights = [1.0] * 12\npreds = [test_predictions, test_predictions2, test_predictions3, test_predictions4, test_predictions5, test_predictions6,\\\n        test_predictions7, test_predictions8, test_predictions9, test_predictions10, test_predictions11, test_predictions12]\nfor i in range(len(weights)):\n    if i == 0:\n        test_predictions = softmax(preds[i]) * weights[i]\n    else:\n        test_predictions += softmax(preds[i]) * weights[i]\n\npredictions_as_ids = np.argsort(-test_predictions, 1)\n\npredictions_as_answer_letters = np.array(list('ABCDE'))[predictions_as_ids]\n\npredictions_as_string = test_df3['prediction'] = [' '.join(row) for row in predictions_as_answer_letters[:, :3]]","metadata":{"execution":{"iopub.status.busy":"2023-09-23T19:48:26.196993Z","iopub.execute_input":"2023-09-23T19:48:26.19741Z","iopub.status.idle":"2023-09-23T19:48:26.206876Z","shell.execute_reply.started":"2023-09-23T19:48:26.197371Z","shell.execute_reply":"2023-09-23T19:48:26.205707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = test_df3[['id', 'prediction']]\nsubmission.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-09-23T19:48:31.393104Z","iopub.execute_input":"2023-09-23T19:48:31.393495Z","iopub.status.idle":"2023-09-23T19:48:31.402676Z","shell.execute_reply.started":"2023-09-23T19:48:31.393463Z","shell.execute_reply":"2023-09-23T19:48:31.401586Z"},"trusted":true},"execution_count":null,"outputs":[]}]}